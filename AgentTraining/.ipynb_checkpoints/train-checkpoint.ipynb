{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RL model on Poker Hand Selection Task \n",
    "\n",
    "## Import packages and configure environment\n",
    "Run tests first in the test notebooks if you face errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.dvipnghack\" on line 127 in\n",
      "/Users/Adam.Massachi@ibm.com/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.2.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"nbagg.transparent\" on line 433 in\n",
      "/Users/Adam.Massachi@ibm.com/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.2.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"animation.mencoder_path\" on line 516 in\n",
      "/Users/Adam.Massachi@ibm.com/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.2.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key \"animation.mencoder_args\" on line 519 in\n",
      "/Users/Adam.Massachi@ibm.com/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.2.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# environment\n",
    "import gym\n",
    "import HandMakerEnv\n",
    "import treys\n",
    "import numpy as np\n",
    "\n",
    "# agent and training\n",
    "import stable_baselines\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "# plotting and logging\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec, NormalActionNoise\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] STARTING TRAINING: Sat-Apr-11-16:40:29-2020 HandMakerEnv-v1-<class 'stable_baselines.common.policies.MlpPolicy'>\n",
      "[INFO] TIMESTEPS 10000\n"
     ]
    }
   ],
   "source": [
    "POLICY = MlpPolicy\n",
    "ENVIRONMENT = 'HandMakerEnv-v1'\n",
    "TIMESTEPS = 10000\n",
    "NETWORK_ARCH = [8192,8192,8192]\n",
    "LOG_INTERVAL = 250\n",
    "\n",
    "START_TIME = time.asctime().replace(' ', '-')\n",
    "TENSORBOARD_DIR = f'./logs/{ENVIRONMENT}-{POLICY}-{START_TIME}-tensorboard'\n",
    "MODEL_DIR = f'./models/{ENVIRONMENT}-{POLICY}-{START_TIME}-{TIMESTEPS}-model-folder.zip'\n",
    "LOAD_EXISTING_MODEL = False\n",
    "LOAD_DIR = None\n",
    "\n",
    "def train(policy=POLICY, environment=ENVIRONMENT, timesteps=TIMESTEPS, load_existing=LOAD_EXISTING_MODEL, log_interval=LOG_INTERVAL):\n",
    "    \n",
    "    print(f\"[INFO] STARTING TRAINING: {START_TIME} {ENVIRONMENT}-{POLICY}\")\n",
    "    print(f\"[INFO] TIMESTEPS {TIMESTEPS}\")\n",
    "    \n",
    "    # configure the environment \n",
    "    env = gym.make(ENVIRONMENT)\n",
    "    \n",
    "    print(f\"[INFO] LOAD EXISTING MODEL? {LOAD_EXISTING_MODEL}\")\n",
    "    if LOAD_EXISTING_MODEL: \n",
    "        model.load(LOAD_DIR)\n",
    "        print(f\"[INFO] LOADED MODEL FROM {LOAD_DIR}\")\n",
    "\n",
    "    else:  \n",
    "        # Custom MLP policy of two layers of size 32 each with tanh activation function\n",
    "        policy_kwargs = dict(net_arch=NETWORK_ARCH)\n",
    "        model = PPO2(policy, env, verbose=0, policy_kwargs=policy_kwargs)\n",
    "        print(f\"[INFO] NEW MODEL {model}\")\n",
    "        \n",
    "    model.learn(total_timesteps=timesteps, log_interval=LOG_INTERVAL)\n",
    "    \n",
    "    model.save(save_path=MODEL_DIR, cloudpickle=False)\n",
    "    print(f\"[INFO] MODEL SAVED TO {MODEL_DIR}\")\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the average reward for random actions? \n",
    "\n",
    "def get_mean_reward_random():\n",
    "    env = gym.make(ENVIRONMENT)\n",
    "    \n",
    "    reward_sum = 0\n",
    "    for _ in range(10000):\n",
    "        obs, r, done, _ = env.step(env.action_space.sample())\n",
    "        reward_sum+=r\n",
    "    return reward_sum/10000\n",
    "\n",
    "def evaluate(model, num_steps=1000):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_steps: (int) number of timesteps to evaluate it\n",
    "    :return: (float) Mean reward for the last 100 episodes\n",
    "    \"\"\"\n",
    "    env = gym.make(ENVIRONMENT)\n",
    "    for t in range(3):\n",
    "        episode_rewards = 0\n",
    "        obs = env.reset()\n",
    "        for i in range(num_steps):\n",
    "          # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards += reward\n",
    "\n",
    "            obs = env.reset()\n",
    "\n",
    "        # Compute mean reward for the last 100000 episodes\n",
    "        print(f\"Mean reward, trial {t}:\", episode_rewards/10000)\n",
    "            \n",
    "\n",
    "# print(\"Mean reward random: \", get_mean_reward_random())\n",
    "# model.load(load_path='models/HandMakerEnv-v1-MlpPolicy-Sat-Apr-11-14:17:49-2020-model-folder.zip')\n",
    "\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cards = [treys.Card.new(f\"{n}h\") for n in [2,3,4,5,6]] + [treys.Card.new(f\"{n}d\") for n in [2,3,4,5,7]] + [treys.Card.new(f\"{n}c\") for n in [2, 6, 8]]\n",
    "flush_obs = np.array([item for sublist in [[int(i) for i in y] for y in [f'{a:032b}' for a in flush_cards]] for item in sublist])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENVIRONMENT)\n",
    "env.card_ints = flush_cards\n",
    "action, _states = model.predict(flush_obs)\n",
    "obs, r, done, info = env.step(action)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MlpPolicy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
