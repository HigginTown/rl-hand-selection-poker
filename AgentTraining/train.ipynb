{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RL model on Poker Hand Selection Task \n",
    "\n",
    "## Import packages and configure environment\n",
    "Run tests first in the test notebooks if you face errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# environment\n",
    "import gym\n",
    "import HandMakerEnv\n",
    "import treys\n",
    "import numpy as np\n",
    "\n",
    "# agent and training\n",
    "import stable_baselines\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "# plotting and logging\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.noise import AdaptiveParamNoiseSpec, NormalActionNoise\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] STARTING TRAINING: Sat-Apr-11-14:54:26-2020 HandMakerEnv-v1-MlpPolicy\n",
      "[INFO] TIMESTEPS 50000\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0005621726 |\n",
      "| clipfrac           | 0.001953125  |\n",
      "| explained_variance | -1.09        |\n",
      "| fps                | 217          |\n",
      "| n_updates          | 1            |\n",
      "| policy_entropy     | 9.010526     |\n",
      "| policy_loss        | -0.016644834 |\n",
      "| serial_timesteps   | 128          |\n",
      "| time_elapsed       | 1.79e-05     |\n",
      "| total_timesteps    | 128          |\n",
      "| value_loss         | 0.42558858   |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "POLICY = 'MlpPolicy'\n",
    "ENVIRONMENT = 'HandMakerEnv-v1'\n",
    "TIMESTEPS = 1000000\n",
    "LOG_INTERVAL = 2500\n",
    "\n",
    "START_TIME = time.asctime().replace(' ', '-')\n",
    "TENSORBOARD_DIR = f'./logs/{ENVIRONMENT}-{POLICY}-{START_TIME}-tensorboard'\n",
    "MODEL_DIR = f'./models/{ENVIRONMENT}-{POLICY}-{START_TIME}-model-folder.zip'\n",
    "LOAD_EXISTING_MODEL = False\n",
    "LOAD_DIR = None\n",
    "\n",
    "def train(policy=POLICY, environment=ENVIRONMENT, timesteps=TIMESTEPS, load_existing=LOAD_EXISTING_MODEL, log_interval=LOG_INTERVAL):\n",
    "    \n",
    "    print(f\"[INFO] STARTING TRAINING: {START_TIME} {ENVIRONMENT}-{POLICY}\")\n",
    "    print(f\"[INFO] TIMESTEPS {TIMESTEPS}\")\n",
    "    \n",
    "    # configure the environment \n",
    "    env = gym.make(ENVIRONMENT)\n",
    "    \n",
    "    if LOAD_EXISTING_MODEL: model.load(LOAD_DIR)\n",
    "    else:  model = PPO2(policy, env, verbose=0)\n",
    "        \n",
    "    model.learn(total_timesteps=timesteps, log_interval=LOG_INTERVAL)\n",
    "    \n",
    "    model.save(save_path=MODEL_DIR, cloudpickle=False)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward, trial 0: 0.024554301795765197\n",
      "Mean reward, trial 1: 0.024332216563923918\n",
      "Mean reward, trial 2: 0.0243640444920933\n",
      "Five %: 0.9983333333333333\n"
     ]
    }
   ],
   "source": [
    "# what is the average reward for random actions? \n",
    "\n",
    "def get_mean_reward_random():\n",
    "    env = gym.make(ENVIRONMENT)\n",
    "    \n",
    "    reward_sum = 0\n",
    "    for _ in range(10000):\n",
    "        obs, r, done, _ = env.step(env.action_space.sample())\n",
    "        reward_sum+=r\n",
    "    return reward_sum/10000\n",
    "\n",
    "def evaluate(model, num_steps=1000):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_steps: (int) number of timesteps to evaluate it\n",
    "    :return: (float) Mean reward for the last 100 episodes\n",
    "    \"\"\"\n",
    "    env = gym.make(ENVIRONMENT)\n",
    "    for t in range(3):\n",
    "        episode_rewards = 0\n",
    "        obs = env.reset()\n",
    "        for i in range(num_steps):\n",
    "          # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards += reward\n",
    "\n",
    "            obs = env.reset(mode)\n",
    "\n",
    "        # Compute mean reward for the last 100000 episodes\n",
    "        print(f\"Mean reward, trial {t}:\", episode_rewards/10000)\n",
    "            \n",
    "\n",
    "# print(\"Mean reward random: \", get_mean_reward_random())\n",
    "# model.load(load_path='models/HandMakerEnv-v1-MlpPolicy-Sat-Apr-11-14:17:49-2020-model-folder.zip')\n",
    "\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "flush_cards = [treys.Card.new(f\"{n}h\") for n in [2,3,4,5,6]] + [treys.Card.new(f\"{n}d\") for n in [2,3,4,5,7]] + [treys.Card.new(f\"{n}c\") for n in [2, 'Q', 'J']]\n",
    "flush_obs = np.array([item for sublist in [[int(i) for i in y] for y in [f'{a:032b}' for a in flush_cards]] for item in sublist])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17180380595014744\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENVIRONMENT)\n",
    "env.card_ints = flush_cards\n",
    "action, _states = model.predict(flush_obs)\n",
    "obs, r, done, info = env.step(action)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
