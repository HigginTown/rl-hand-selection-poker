{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RL model on Poker Hand Selection Task \n",
    "\n",
    "## Import packages and configure environment\n",
    "Run tests first in the test notebooks if you face errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# environment\n",
    "import gym\n",
    "import HandClassificationEnv\n",
    "import treys\n",
    "import numpy as np\n",
    "\n",
    "# agent and training\n",
    "import stable_baselines\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "# plotting and logging\n",
    "from stable_baselines.bench import Monitor\n",
    "# from stable_baselines.results_plotter import load_results, ts2xy\n",
    "# from stable_baselines.common.noise import AdaptiveParamNoiseSpec, NormalActionNoise\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "# import matplotlib.pyplot as plt\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] STARTING TRAINING: Sun-Apr-12-15:03:13-2020 HandClassificationEnv-v1-MlpPolicy-PPO2\n",
      "[INFO] NETWORK ARCH [160, 160, 160, 160]\n",
      "[INFO] LOAD EXISTING MODEL? False\n",
      "[INFO] Training for TIMESTEPS 250000\n",
      "[INFO] MODEL SAVED TO ./models/HandClassificationEnv-v1-MlpPolicy-PPO2-Sun-Apr-12-15:03:13-2020-250000-[160, 160, 160, 160]-model-folder\n"
     ]
    }
   ],
   "source": [
    "POLICY = MlpPolicy\n",
    "POLICY_NAME = 'MlpPolicy'\n",
    "ENVIRONMENT = 'HandClassificationEnv-v1'\n",
    "TIMESTEPS = 250000\n",
    "NETWORK_ARCH = [160,160,160,160]\n",
    "LOG_INTERVAL = 2500\n",
    "\n",
    "START_TIME = time.asctime().replace(' ', '-')\n",
    "TENSORBOARD_DIR = f'./logs/tb/{ENVIRONMENT}-{POLICY_NAME}-{START_TIME}-tensorboard'\n",
    "MODEL_DIR = f'./models/{ENVIRONMENT}-{POLICY_NAME}-PPO2-{START_TIME}-{TIMESTEPS}-{NETWORK_ARCH}-model-folder'\n",
    "LOAD_EXISTING_MODEL = False\n",
    "LOAD_DIR = None\n",
    "\n",
    "def train(policy=POLICY, environment=ENVIRONMENT, timesteps=TIMESTEPS, load_existing=LOAD_EXISTING_MODEL, log_interval=LOG_INTERVAL):\n",
    "    \n",
    "    print(f\"[INFO] STARTING TRAINING: {START_TIME} {ENVIRONMENT}-{POLICY_NAME}-PPO2\")\n",
    "    print(f\"[INFO] NETWORK ARCH {NETWORK_ARCH}\")\n",
    "    \n",
    "    # configure the environment \n",
    "    env = gym.make(ENVIRONMENT)\n",
    "    \n",
    "    print(f\"[INFO] LOAD EXISTING MODEL? {LOAD_EXISTING_MODEL}\")\n",
    "    if LOAD_EXISTING_MODEL: \n",
    "        model.load(LOAD_DIR)\n",
    "        print(f\"[INFO] LOADED MODEL FROM {LOAD_DIR}\")\n",
    "\n",
    "    else:  \n",
    "        # Custom MLP policy of two layers of size 32 each with tanh activation function\n",
    "        policy_kwargs = dict(net_arch=NETWORK_ARCH)\n",
    "        model = PPO2(policy, env, verbose=0, policy_kwargs=policy_kwargs)\n",
    "        print(f\"[INFO] Training for TIMESTEPS {TIMESTEPS}\")\n",
    "        \n",
    "    model.learn(total_timesteps=timesteps, log_interval=LOG_INTERVAL, tb_log_name=TENSORBOARD_DIR)\n",
    "    \n",
    "    model.save(save_path=MODEL_DIR, cloudpickle=False)\n",
    "    print(f\"[INFO] MODEL SAVED TO {MODEL_DIR}\")\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the average reward for random actions? \n",
    "from collections import Counter\n",
    "\n",
    "def get_mean_reward_random():\n",
    "    env = gym.make(ENVIRONMENT)\n",
    "    \n",
    "    reward_sum = 0\n",
    "    for i in range(10000):\n",
    "        random_action = env.action_space.sample()\n",
    "        obs, r, done, _ = env.step(random_action)\n",
    "        reward_sum+=r\n",
    "        env.reset()\n",
    "            \n",
    "    return reward_sum/10000\n",
    "\n",
    "print(f\"Mean reward 10 000 trails with random action: {get_mean_reward_random()} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward 10 000 trails with random action: -0.7788 \n",
      "\n",
      "[INFO] Mean reward: 0.84556 \n",
      "\n",
      "[INFO] Predictions distribution: Counter({8: 25524, 7: 24475, 1: 1}) \n",
      "\n",
      "[INFO] Actual states distribution: Counter({8: 25143, 7: 21091, 6: 2368, 5: 1028, 4: 209, 3: 88, 2: 64, 1: 8, 0: 1}) \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.00      0.00      0.00         8\n",
      "          2       0.00      0.00      0.00        64\n",
      "          3       0.00      0.00      0.00        88\n",
      "          4       0.00      0.00      0.00       209\n",
      "          5       0.00      0.00      0.00      1028\n",
      "          6       0.00      0.00      0.00      2368\n",
      "          7       0.86      1.00      0.92     21091\n",
      "          8       0.99      1.00      0.99     25143\n",
      "\n",
      "avg / total       0.86      0.92      0.89     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = f\"./logs/{MODEL_DIR}/evaluation.txt\"\n",
    "def evaluate(model, num_steps=50000, log_dir=LOG_DIR):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_steps: (int) number of timesteps to evaluate it\n",
    "    :param log_dir: (str) where to write the classification report\n",
    "    \"\"\"\n",
    "    \n",
    "    env = gym.make(ENVIRONMENT)\n",
    "    obs = env._get_obs()\n",
    "\n",
    "    episode_rewards = 0\n",
    "    predictions = []\n",
    "    actual_states = []\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        # store the observation for evaluation later \n",
    "        actual_states.append(env.rank_class-1)\n",
    "        # make a prediction\n",
    "        action, _states = model.predict(obs)\n",
    "        # observer the reward\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # store the reward for analysis later\n",
    "        episode_rewards += reward\n",
    "        # store the action for analysis later \n",
    "        predictions.append(action)\n",
    "\n",
    "        # get a new observation for a new prediction\n",
    "        obs = env.reset()\n",
    "\n",
    "    # Compute mean reward for the last 100000 episodes\n",
    "    # and save it to a file \n",
    "    import os\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    filename = LOG_DIR\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(f\"[INFO] Mean reward: {episode_rewards/num_steps} \\n\")\n",
    "        print(f\"[INFO] Mean reward: {episode_rewards/num_steps} \\n\")\n",
    "        print(f\"[INFO] Predictions distribution: {Counter(predictions)} \\n\")\n",
    "        print(f\"[INFO] Actual states distribution: {Counter(actual_states)} \\n\")\n",
    "        f.write(f\"{classification_report(actual_states, predictions)}\")\n",
    "        print(classification_report(actual_states, predictions))\n",
    "        \n",
    "\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
