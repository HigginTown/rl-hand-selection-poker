{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RL model on Poker Hand Selection Task \n",
    "\n",
    "## Import packages and configure environment\n",
    "Run tests first in the test notebooks if you face errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# environment\n",
    "import gym\n",
    "import HandClassificationEnv\n",
    "import treys\n",
    "import numpy as np\n",
    "\n",
    "# agent and training\n",
    "import stable_baselines\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "# plotting and logging\n",
    "from stable_baselines.bench import Monitor\n",
    "# from stable_baselines.results_plotter import load_results, ts2xy\n",
    "# from stable_baselines.common.noise import AdaptiveParamNoiseSpec, NormalActionNoise\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "# import matplotlib.pyplot as plt\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] STARTING TRAINING: Sat-Apr-11-18:54:52-2020 HandClassificationEnv-v1-MlpPolicy-PPO2\n",
      "[INFO] TIMESTEPS 2000000\n",
      "[INFO] NETWORK ARCH [160, 160, 160, 160]\n",
      "[INFO] LOAD EXISTING MODEL? False\n",
      "[INFO] NEW MODEL <stable_baselines.ppo2.ppo2.PPO2 object at 0x146b99828>\n"
     ]
    }
   ],
   "source": [
    "POLICY = MlpPolicy\n",
    "POLICY_NAME = 'MlpPolicy'\n",
    "ENVIRONMENT = 'HandClassificationEnv-v1'\n",
    "TIMESTEPS = 2000000\n",
    "NETWORK_ARCH = [160,160,160,160]\n",
    "LOG_INTERVAL = 2000\n",
    "\n",
    "START_TIME = time.asctime().replace(' ', '-')\n",
    "TENSORBOARD_DIR = f'./logs/{ENVIRONMENT}-{POLICY_NAME}-{START_TIME}-tensorboard'\n",
    "MODEL_DIR = f'./models/{ENVIRONMENT}-{POLICY_NAME}-PPO2-{START_TIME}-{TIMESTEPS}-{NETWORK_ARCH}-model-folder.zip'\n",
    "LOAD_EXISTING_MODEL = False\n",
    "LOAD_DIR = None\n",
    "\n",
    "def train(policy=POLICY, environment=ENVIRONMENT, timesteps=TIMESTEPS, load_existing=LOAD_EXISTING_MODEL, log_interval=LOG_INTERVAL):\n",
    "    \n",
    "    print(f\"[INFO] STARTING TRAINING: {START_TIME} {ENVIRONMENT}-{POLICY_NAME}-PPO2\")\n",
    "    print(f\"[INFO] NETWORK ARCH {NETWORK_ARCH}\")\n",
    "    \n",
    "    # configure the environment \n",
    "    env = gym.make(ENVIRONMENT)\n",
    "    \n",
    "    print(f\"[INFO] LOAD EXISTING MODEL? {LOAD_EXISTING_MODEL}\")\n",
    "    if LOAD_EXISTING_MODEL: \n",
    "        model.load(LOAD_DIR)\n",
    "        print(f\"[INFO] LOADED MODEL FROM {LOAD_DIR}\")\n",
    "\n",
    "    else:  \n",
    "        # Custom MLP policy of two layers of size 32 each with tanh activation function\n",
    "        policy_kwargs = dict(net_arch=NETWORK_ARCH)\n",
    "        model = PPO2(policy, env, verbose=0, policy_kwargs=policy_kwargs)\n",
    "        print(f\"[INFO] Training for TIMESTEPS {TIMESTEPS})\n",
    "        \n",
    "    model.learn(total_timesteps=timesteps, log_interval=LOG_INTERVAL)\n",
    "    \n",
    "    model.save(save_path=MODEL_DIR, cloudpickle=False)\n",
    "    print(f\"[INFO] MODEL SAVED TO {MODEL_DIR}\")\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Random:  -0.896\n",
      "[INFO] Mean reward, trial 0: -0.0134\n",
      "[INFO] Predictions distribution: Counter({8: 947, 7: 46, 6: 4, 2: 2, 0: 1})\n",
      "[INFO] Mean reward, trial 1: -0.0106\n",
      "[INFO] Predictions distribution: Counter({8: 955, 7: 41, 6: 3, 2: 1})\n",
      "[INFO] Mean reward, trial 2: -0.0096\n",
      "[INFO] Predictions distribution: Counter({8: 959, 7: 39, 6: 1, 2: 1})\n"
     ]
    }
   ],
   "source": [
    "# what is the average reward for random actions? \n",
    "from collections import Counter\n",
    "\n",
    "def get_mean_reward_random():\n",
    "    env = gym.make(ENVIRONMENT)\n",
    "    \n",
    "    reward_sum = 0\n",
    "    for i in range(10000):\n",
    "        random_action = env.action_space.sample()\n",
    "        obs, r, done, _ = env.step(random_action)\n",
    "        reward_sum+=r\n",
    "        env.reset()\n",
    "            \n",
    "    return reward_sum/10000\n",
    "\n",
    "print(\"Mean Random: \", get_mean_reward_random())\n",
    "\n",
    "def evaluate(model, num_steps=1000):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_steps: (int) number of timesteps to evaluate it\n",
    "    :return: (float) Mean reward for the last 100 episodes\n",
    "    \"\"\"\n",
    "    env = gym.make(ENVIRONMENT)\n",
    "    obs = env._get_obs()\n",
    "    for t in range(3):\n",
    "        episode_rewards = 0\n",
    "        predictions = []\n",
    "        for i in range(num_steps):\n",
    "          # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards += reward\n",
    "            predictions.append(action)\n",
    "\n",
    "            obs = env.reset()\n",
    "\n",
    "        # Compute mean reward for the last 100000 episodes\n",
    "        print(f\"[INFO] Mean reward, trial {t}:\", episode_rewards/10000)\n",
    "        print(f\"[INFO] Predictions distribution: {Counter(predictions)}\")\n",
    "            \n",
    "\n",
    "# print(\"Mean reward random: \", get_mean_reward_random())\n",
    "# model.load(load_path='models/HandMakerEnv-v1-MlpPolicy-Sat-Apr-11-14:17:49-2020-model-folder.zip')\n",
    "\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
